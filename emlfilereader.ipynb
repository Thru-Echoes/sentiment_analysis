{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a328dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/echoes/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "#%pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#%pip install --upgrade pip\n",
    "\n",
    "#%pip install requests\n",
    "import requests\n",
    "\n",
    "#%pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#this is how I imported the library i've been using to extract keywords\n",
    "#I have not been able to get it to work yet (see extract_keywords_from_text function)\n",
    "#if anyone has suggestions or other libraries that might work better, please let me know!\n",
    "\n",
    "#this was copied from https://stackoverflow.com/questions/38916452/nltk-download-ssl-certificate-verify-failed\n",
    "#correcting an error that sometimes occurs when downloading nltk functions\n",
    "\n",
    "#%pip install rake-nltk\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82885060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_in_file(file_name):\n",
    "    \"\"\" Takes an .eml file name as a string and returns a list of all unique urls linking to news sources \"\"\"\n",
    "\n",
    "    #reads the file and removes newlines and equal signs (which are placed at the end of each line)\n",
    "    with open(file_name, 'r') as file:\n",
    "        textfile = file.read().replace('\\n', '').replace('=', '')\n",
    "    \n",
    "    #any strings without white spaces starting with 3Dhttps:// and ending with &amp, non-greedy matching\n",
    "    valid_urls = re.findall('3D(https://\\S*?)?&amp', textfile)\n",
    "    \n",
    "    #creates a list of each unique url (removes duplicates)\n",
    "    unique_urls = list(set(valid_urls))\n",
    "    \n",
    "    return unique_urls\n",
    "\n",
    "#demonstrates the function with the first file in the folder\n",
    "#urls_in_file('eml_files/1. El Salvador Mineria 1-94 eml/Google Alert - El Salvador mineriÃÅa_1.eml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce63e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_in_folder(folder_name):\n",
    "    \"\"\" Takes a folder of .eml files as a string and returns a list of all unique urls contained in each of the \n",
    "    files in the folder \n",
    "\"\"\"\n",
    "    \n",
    "    #creates a list of all file names in folder_name\n",
    "    filenames = os.listdir(folder_name)\n",
    "    filenames = [os.path.join(folder_name, file) for file in filenames]\n",
    "    \n",
    "    #creates a list to store the unique urls found in each file using urls_in_file function\n",
    "    all_urls = []\n",
    "    for file in filenames:\n",
    "        urls = urls_in_file(file)\n",
    "        all_urls.extend(urls)\n",
    "    \n",
    "    return all_urls\n",
    "\n",
    "#demonstrates the function with folder '1. El Salvador Mineria 1-94 eml'\n",
    "#urls_in_folder('eml_files/1. El Salvador Mineria 1-94 eml')[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17ba37",
   "metadata": {},
   "source": [
    "## Helpful Utility Functions \n",
    "\n",
    "**Try to integrate these functions into your code.** \n",
    "\n",
    "### Enhanced header date parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a934366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.utils import parsedate_to_datetime\n",
    "from datetime import timezone \n",
    "\n",
    "def extract_date_header(msg):\n",
    "    # Need to parse the Date header from Google Alert \n",
    "    date_header = msg.get(\"Date\")\n",
    "\n",
    "    if not date_header:\n",
    "        print(\"[WARN] date was not extracted.\")\n",
    "        return None \n",
    "\n",
    "    try:\n",
    "        dt = parsedate_to_datetime(date_header) # try to parse the header into a datetime object \n",
    "        if dt is None:\n",
    "            print(\"[WARN] date header not able to turn into datetime object\")\n",
    "            return None \n",
    "        return dt.astimezone(timezone.utc).isoformat()\n",
    "    except Exception:\n",
    "        print(\"Exception occurred!\")\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeef149",
   "metadata": {},
   "source": [
    "### Unwrap Google redirect URLs \n",
    "\n",
    "Google will give you URLs that look like: \n",
    "\n",
    "*www.google.com/url?...&url=www.example.com* \n",
    "\n",
    "The unwrap_google_url() function below will extract that \"www.example.com\" from the whole Google redirect URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ea43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs, unquote \n",
    "\n",
    "def unwrap_google_url(google_redirect):\n",
    "    if not google_redirect:\n",
    "        return google_redirect\n",
    "    \n",
    "    parsed = urlparse(google_redirect)    # this breaks URL into components \n",
    "\n",
    "    if parsed.netloc.endswith(\"google.com\") and parsed.path.strip(\"/\").lower() == \"url\":\n",
    "        # if above returns True, then it is google redirect \n",
    "        url_query = parse_qs(parsed.query)\n",
    "\n",
    "        if \"url\" in url_query and url_query[\"url\"]:\n",
    "            return unquote(url_query[\"url\"][0])\n",
    "    return google_redirect   # if it is not a wrapper for another URL, then just return it (because it is not redirect!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142da46",
   "metadata": {},
   "source": [
    "### Get (plan_text, html_text) from Email Message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bodies(msg):\n",
    "    \"\"\"Return (plain_text, html_text) from an EmailMessage; warn if missing.\"\"\"\n",
    "    plain_text, html_text = None, None\n",
    "    try:\n",
    "        p = msg.get_body(preferencelist=('plain',))\n",
    "        if p: plain_text = p.get_content()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] plain body decode failed: {e}\")\n",
    "    try:\n",
    "        h = msg.get_body(preferencelist=('html',))\n",
    "        if h: html_text = h.get_content()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] html body decode failed: {e}\")\n",
    "    if plain_text is None: print(\"[WARN] No text/plain part found.\")\n",
    "    if html_text  is None: print(\"[WARN] No text/html part found.\")\n",
    "    return plain_text, html_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89554e",
   "metadata": {},
   "source": [
    "### Get inboxmarkup JSON \n",
    "\n",
    "JSON = javascript object notation. It is extremely widely used as a file type and data type. It integrates very well with many languages, including Python (and obviously Javascript as well). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a298d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def extract_inboxmarkup_json(html):\n",
    "    \"\"\"Return dict from <script data-scope='inboxmarkup' type='application/json'>, else None.\"\"\"\n",
    "    if not html:\n",
    "        print(\"[WARN] No HTML body; cannot extract inboxmarkup JSON.\")\n",
    "        return None\n",
    "    m = re.search(\n",
    "        r'<script[^>]*data-scope=[\"\\']inboxmarkup[\"\\'][^>]*type=[\"\\']application/json[\"\\'][^>]*>\\s*({.*?})\\s*</script>',\n",
    "        html, flags=re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if not m:\n",
    "        print(\"[WARN] inboxmarkup JSON not found in HTML.\")\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(1))\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return json.loads(m.group(1).replace(\"\\\\u0026\", \"&\"))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] inboxmarkup JSON parse failed: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cb64b",
   "metadata": {},
   "source": [
    "### EML file parser \n",
    "\n",
    "This is a major function that will help a lot! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca87ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from email.utils import parsedate_to_datetime\n",
    "from datetime import timezone\n",
    "import pandas as pd\n",
    "\n",
    "def parse_eml_file(eml_path):\n",
    "    \"\"\"Return {'email_meta': {...}, 'items': [...] } for one .eml file.\"\"\"\n",
    "    with open(eml_path, \"rb\") as f:\n",
    "        msg = BytesParser(policy=policy.default).parse(f)\n",
    "\n",
    "    # date header ‚Üí ISO UTC\n",
    "    raw_date = msg.get(\"Date\")\n",
    "    date_iso = None\n",
    "    if raw_date:\n",
    "        try:\n",
    "            date_iso = parsedate_to_datetime(raw_date).astimezone(timezone.utc).isoformat()\n",
    "        except Exception:\n",
    "            print(f\"[WARN] Could not parse Date header: {raw_date!r}\")\n",
    "    else:\n",
    "        print(\"[WARN] Date header missing.\")\n",
    "\n",
    "    # bodies + inboxmarkup\n",
    "    plain_text, html_text = get_bodies(msg)\n",
    "    inbox = extract_inboxmarkup_json(html_text or \"\")\n",
    "\n",
    "    # items\n",
    "    items = []\n",
    "    entity_title, latest_subtitle, latest_iso = None, None, None\n",
    "    if inbox:\n",
    "        ent = inbox.get(\"entity\") or {}\n",
    "        entity_title = ent.get(\"title\")\n",
    "        latest_subtitle = ent.get(\"subtitle\")\n",
    "        if latest_subtitle:\n",
    "            # e.g., \"Latest: March 31, 2025\"\n",
    "            raw = latest_subtitle.split(\":\", 1)[-1].strip()\n",
    "            ts = pd.to_datetime(raw, errors=\"coerce\")\n",
    "            if pd.notna(ts):\n",
    "                latest_iso = (ts.tz_localize(\"UTC\").isoformat()\n",
    "                              if ts.tzinfo is None else ts.tz_convert(\"UTC\").isoformat())\n",
    "        for card in inbox.get(\"cards\", []):\n",
    "            for w in card.get(\"widgets\", []):\n",
    "                if w.get(\"type\") == \"LINK\":\n",
    "                    g = w.get(\"url\")\n",
    "                    r = unwrap_google_url(g) if g else None\n",
    "                    items.append({\n",
    "                        \"eml_path\": str(eml_path),\n",
    "                        \"message_id\": msg.get(\"Message-ID\"),\n",
    "                        \"email_date_utc\": date_iso,\n",
    "                        \"json_latest_date_utc\": latest_iso,\n",
    "                        \"entity_title\": entity_title,\n",
    "                        \"item_title\": w.get(\"title\"),\n",
    "                        \"item_description\": w.get(\"description\"),\n",
    "                        \"google_url\": g,\n",
    "                        \"resolved_url\": r\n",
    "                    })\n",
    "    print(f\"[INFO] Parsed {eml_path}: {len(items)} items.\")\n",
    "    return {\n",
    "        \"email_meta\": {\n",
    "            \"eml_path\": str(eml_path),\n",
    "            \"message_id\": msg.get(\"Message-ID\"),\n",
    "            \"subject\": msg.get(\"Subject\"),\n",
    "            \"from\": msg.get(\"From\"),\n",
    "            \"to\": msg.get(\"To\"),\n",
    "            \"list_id\": msg.get(\"List-Id\"),\n",
    "            \"date_email_raw\": raw_date,\n",
    "            \"date_email_utc\": date_iso,\n",
    "            \"entity_title\": entity_title,\n",
    "            \"json_latest_subtitle\": latest_subtitle,\n",
    "            \"json_latest_date_utc\": latest_iso\n",
    "        },\n",
    "        \"items\": items\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f47cf",
   "metadata": {},
   "source": [
    "## Ideas for language processing \n",
    "\n",
    "- simple bilingual lexicons \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e385944d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bitcoin', 'El Salvador']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_keywords_from_eml(file_name):\n",
    "    \"\"\" \n",
    "    Takes an .eml file name as a string and returns a list of the keywords used in the Google Alerts\n",
    "    that generated the file. \n",
    "\n",
    "    Only works with Google Alert .eml files with an ENGLISH subject line, which are formatted as:\n",
    "    Subject: Google Alert - \"keyword1\" \"keyword2\" \"keyword3\"\n",
    "\n",
    "    Google Alerts in Spanish appear to be formatted with UTF-8 encoding, which results in subject lines like:\n",
    "    Subject: =?UTF-8?Q?Google_Alert_=2D_El_Salvador_miner=C3=ADa?=\n",
    "\n",
    "    NOTE: I chose to use the actual eml file text rather than the name of the file because I didn't want\n",
    "    the function to be dependent on the file name format, which might vary in the future.\n",
    "    \"\"\"   \n",
    "    \n",
    "    #reads the file and removes equal signs (which are placed at the end of each line)\n",
    "    with open(file_name, 'r') as file:\n",
    "        textfile = file.read().replace('=', '')\n",
    "    \n",
    "    #creates a list of keywords found in the textfile\n",
    "    pattern = 'Subject: Google Alert - (.*)' \n",
    "    keyword_block = re.findall(pattern, textfile)\n",
    "    keywords = keyword_block[0].split('\" \"')\n",
    "    keywords = [keywords[i].replace('\"', '').replace(\"'\", \"\") for i in range(len(keywords))]\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "#demonstrates function with Google Alert - Bitcoin_ _El Salvador__1.eml\n",
    "extract_keywords_from_eml('eml_files/Google Alert - _Bitcoin_ _El Salvador__1.eml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ab3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/var/folders/yv/y95m0ts51pl7zc59py5b_ckm0000gn/T/ipykernel_62254/769742125.py\u001b[39m(\u001b[92m27\u001b[39m)\u001b[36mextract_website_details\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     25\u001b[39m         \u001b[38;5;66;03m#Extracts date from webpage, if available\u001b[39;00m\n",
      "\u001b[32m     26\u001b[39m         \u001b[38;5;66;03m#This part of the function isn't working yet, so it always returns 'Date not found'\u001b[39;00m\n",
      "\u001b[32m---> 27\u001b[39m         date = soup.find(\u001b[33m\"span\"\u001b[39m, class_=\u001b[33m\"post-date\"\u001b[39m).get_text(strip=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m soup.find(\u001b[33m\"span\"\u001b[39m, class_=\u001b[33m\"post-date\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'Date not found'\u001b[39m\n",
      "\u001b[32m     28\u001b[39m \n",
      "\u001b[32m     29\u001b[39m         \u001b[38;5;66;03m#Creating a BeautifulSoup object takes significant time, so this function extracts all neccessary information in one go\u001b[39;00m\n",
      "\n",
      "'Muro colaps√≥ tras sismo en San Salvador y dej√≥ a una persona herida'\n",
      "'Muro colaps√≥ tras sismo en San Salvador y dej√≥ a una persona herida'\n",
      "['Una persona qued√≥ atrapada tras el colapso de un muro en San Salvador, provocado por un sismo. Fue rescatada y atendida por Cruz Verde Salvadore√±a.', 'Una persona qued√≥ atrapada bajo los escombros de un muro que colaps√≥ en la colonia Las Flores, calle Agua Caliente, San Salvador Este, tras el sismo registrado la tarde del lunes 5 de enero a las 5:37 p. m.; La v√≠ctima, cuya identidad no ha sido revelada, fue localizada por un residente del lugar y posteriormente rescatada por elementos de Cruz Verde Salvadore√±a, quienes le brindaron primeros auxilios antes de trasladarla a un centro asistencial.', 'Este sismo fue una de las r√©plicas del movimiento tel√∫rico de magnitud 6.3 que ocurri√≥ frente a la costa de La Paz a las 11:18 a.m. del domingo 5 de enero. Seg√∫n el Ministerio de Medio Ambiente, desde entonces se han registrado 147 r√©plicas, de las cuales 21 han sido sentidas por la poblaci√≥n. Las autoridades explicaron que esta actividad s√≠smica es producto del proceso de subducci√≥n entre las placas de Cocos y Caribe. No se reporta amenaza de tsunami para las costas salvadore√±as.']\n",
      "'Una persona qued√≥ atrapada tras el colapso de un muro en San Salvador, provocado por un sismo. Fue rescatada y atendida por Cruz Verde Salvadore√±a.'\n",
      "'Una persona qued√≥ atrapada tras el colapso de un muro en San Salvador, provocado por un sismo. Fue rescatada y atendida por Cruz Verde Salvadore√±a.'\n",
      "*** NameError: name 'Escape' is not defined\n"
     ]
    }
   ],
   "source": [
    "def extract_website_details_PREVIOUS(url, cutoff=20):\n",
    "    \"\"\"\n",
    "    Takes in a url as a string and returns the text content of the webpage, removing paragraphs shorter than cutoff words.\n",
    "    Returns a list containing the title, date, and body text of the webpage.\n",
    "    If the webpage cannot be accessed, returns a list of None values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') #creates a BeautifulSoup object from the webpage content\n",
    "\n",
    "        #Extracts text from webpage and attempts to remove headers and footers\n",
    "        extracted_text = soup.get_text(separator='\\n', strip=True)\n",
    "        split = extracted_text.split('\\n') #split the text into a list divided every time there is a new line \\n\n",
    "        body = [paragraph for paragraph in split if len(paragraph.split()) > cutoff] #remove paragraphs with less than CUTOFF words\n",
    "        body_text = ' '.join(body) #combine remaining text into a single string\n",
    "\n",
    "        #Extracts title from webpage\n",
    "        title = soup.title.string if soup.title else 'Title not found'\n",
    "\n",
    "        # YOU CAN USE PYTHON's NATIVE DEBUGGER TO PAUSE CODE AT A SPECIFIC PLACE TO SEE WHAT IS HAPPENING: \n",
    "        import pdb \n",
    "        pdb.set_trace() \n",
    "\n",
    "        #Extracts date from webpage, if available\n",
    "        #This part of the function isn't working yet, so it always returns 'Date not found'\n",
    "        date = soup.find(\"span\", class_=\"post-date\").get_text(strip=True) if soup.find(\"span\", class_=\"post-date\") else 'Date not found'\n",
    "        \n",
    "        #Creating a BeautifulSoup object takes significant time, so this function extracts all neccessary information in one go\n",
    "        #which is more efficient than creating separate functions for title, date, and body text.\n",
    "\n",
    "        #Returns a list containing the title, date, and body text\n",
    "        return [title, date, body_text]\n",
    "    \n",
    "    # If the webpage cannot be accessed, return a list of None values to fill in the table\n",
    "    # It may be helpful to later fill in the error values instead of None to troubleshoot different problems\n",
    "    except requests.RequestException as e:\n",
    "        # Uncomment the next line to print errors during debugging\n",
    "        #print(f\"Error fetching {url}: {e}\")\n",
    "        return [None, None, None]\n",
    "    \n",
    "\n",
    "#this used to be a separate function, but it is now integrated into extract_website_details\n",
    "#def clean_extracted_text(text, cutoff=20):\n",
    "    #\"\"\"Cleans the text extracted using extract_website_text and removes short paragraphs (headers, footers, etc...) based on word count cutoff\"\"\"\n",
    "    #if text is None or text == [None, None, None]:\n",
    "    #    return [None, None, None]\n",
    "    #else: \n",
    "    #    split = text[2].split('\\n') #split the text into a list divided every time there is a new line \\n\n",
    "    #    body = [paragraph for paragraph in split if len(paragraph.split()) > cutoff] #remove paragraphs with less than CUTOFF words.\n",
    "    #    body_text = ' '.join(body) #combine remaining text into a single string\n",
    "    #    return [text[0], text[1], body_text]\n",
    "\n",
    "#demonstrates the extract_website_details function with an example url\n",
    "example_text = extract_website_details_PREVIOUS('https://www.contrapunto.com.sv/muro-colapso-tras-sismo-en-san-salvador-y-dejo-a-una-persona-herida/')\n",
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3550cca",
   "metadata": {},
   "source": [
    "### UPDATED extract_website_dtails() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bce8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"}\n",
    "\n",
    "def extract_publish_date_from_html(soup, html_text):\n",
    "    \"\"\"Try JSON-LD Article/NewsArticle, meta tags, <time datetime>, else None.\"\"\"\n",
    "    # Try a common meta tag\n",
    "    meta = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "    if meta and meta.get(\"content\"):\n",
    "        return meta[\"content\"]\n",
    "\n",
    "    # Try a simple <time> tag\n",
    "    t = soup.find(\"time\", attrs={\"datetime\": True})\n",
    "    if t and t.get(\"datetime\"):\n",
    "        return t[\"datetime\"]\n",
    "\n",
    "    print(\"[WARN] publish date not found (skeleton).\")\n",
    "    return None\n",
    "\n",
    "def parse_any_date_to_iso(date_str):\n",
    "    \"\"\"Return ISO-8601 (UTC) from various formats; warn if missing.\"\"\"\n",
    "    if not date_str:\n",
    "        print(\"[WARN] date was not extracted.\")\n",
    "        return None\n",
    "\n",
    "    # Minimal parse using pandas if available; otherwise return raw.\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        ts = pd.to_datetime(date_str, errors=\"coerce\", utc=True)\n",
    "        if pd.isna(ts):\n",
    "            print(f\"[WARN] could not parse date string (skeleton): {date_str!r}\")\n",
    "            return None\n",
    "        return ts.isoformat()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] basic parse unavailable ({e}); returning raw date string.\")\n",
    "        return date_str\n",
    "\n",
    "def extract_website_details(url, cutoff=20):\n",
    "    \"\"\"\n",
    "    Fetch URL and return [title, date_iso, body_text].\n",
    "    Keeps only paragraphs with > cutoff words.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers= HEADERS, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[WARN] Request failed for {url}: {e}\")\n",
    "        return [None, None, None]\n",
    "\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    html_text = resp.text\n",
    "\n",
    "    # title with fallback\n",
    "    title = (soup.title.string.strip() if soup.title and soup.title.string else None)\n",
    "    if not title:\n",
    "        og = soup.find(\"meta\", attrs={\"property\":\"og:title\"}) or soup.find(\"meta\", attrs={\"name\":\"title\"})\n",
    "        title = og[\"content\"].strip() if og and og.get(\"content\") else \"Title not found\"\n",
    "        if title == \"Title not found\": print(\"[WARN] title was not extracted.\")\n",
    "\n",
    "    # body text (simple but reliable)\n",
    "    parts = [p.strip() for p in soup.get_text(separator=\"\\n\", strip=True).split(\"\\n\") if p.strip()]\n",
    "    body = \" \".join([p for p in parts if len(p.split()) > cutoff]).strip()\n",
    "    if not body:\n",
    "        print(\"[WARN] body text empty after filtering; consider lowering cutoff.\")\n",
    "\n",
    "    # date\n",
    "    raw_date = extract_publish_date_from_html(soup, html_text)\n",
    "    iso_date = parse_any_date_to_iso(raw_date)\n",
    "    return [title, iso_date, body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3c78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cointelegraph.com/learn/articles/bitco...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://tradersunion.com/news/editors-picks/sh...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.novinite.com/articles/231545/The%2...</td>\n",
       "      <td>The Role of Cryptocurrency in Developing Econo...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>Money, for most people, is something they don‚Äô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://blockchain.news/flashnews/trump-to-mee...</td>\n",
       "      <td>Trump to Meet El Salvador Leader at White Hous...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>According to @rovercrc, former President Trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://crypto.news/trump-plans-white-house-vi...</td>\n",
       "      <td>Trump plans White House visit for El Salvador‚Äô...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>by Bloomberg, follows Bukele‚Äôs agreement to de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.crypto-reporter.com/press-releases...</td>\n",
       "      <td>Bukele Rejects IMF, Keeps Buying BTC, and FXGu...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>El Salvador‚Äôs President Nayib Bukele made news...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://cryptobriefing.com/bitcoin-meeting-whi...</td>\n",
       "      <td>Trump plans to meet with Bitcoin bull Nayib Bu...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>The two pro-Bitcoin leaders have maintained co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.cryptoninjas.net/news/new-legislat...</td>\n",
       "      <td>New Legislation Introduced in Panama to Turn t...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>Panama is set to introduce a comprehensive dra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://news.bitcoin.com/first-tokenized-wareh...</td>\n",
       "      <td>First Tokenized Warehouse Complex Built in El ...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://coincentral.com/trump-to-host-nayib-bu...</td>\n",
       "      <td>Trump to Host Nayib Bukele Following Deportati...</td>\n",
       "      <td>Date not found</td>\n",
       "      <td>Trump plans to host El Salvador's President Bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://cointelegraph.com/learn/articles/bitco...   \n",
       "1  https://tradersunion.com/news/editors-picks/sh...   \n",
       "2  https://www.novinite.com/articles/231545/The%2...   \n",
       "3  https://blockchain.news/flashnews/trump-to-mee...   \n",
       "4  https://crypto.news/trump-plans-white-house-vi...   \n",
       "5  https://www.crypto-reporter.com/press-releases...   \n",
       "6  https://cryptobriefing.com/bitcoin-meeting-whi...   \n",
       "7  https://www.cryptoninjas.net/news/new-legislat...   \n",
       "8  https://news.bitcoin.com/first-tokenized-wareh...   \n",
       "9  https://coincentral.com/trump-to-host-nayib-bu...   \n",
       "\n",
       "                                               title            date  \\\n",
       "0                                               None            None   \n",
       "1                                               None            None   \n",
       "2  The Role of Cryptocurrency in Developing Econo...  Date not found   \n",
       "3  Trump to Meet El Salvador Leader at White Hous...  Date not found   \n",
       "4  Trump plans White House visit for El Salvador‚Äô...  Date not found   \n",
       "5  Bukele Rejects IMF, Keeps Buying BTC, and FXGu...  Date not found   \n",
       "6  Trump plans to meet with Bitcoin bull Nayib Bu...  Date not found   \n",
       "7  New Legislation Introduced in Panama to Turn t...  Date not found   \n",
       "8  First Tokenized Warehouse Complex Built in El ...  Date not found   \n",
       "9  Trump to Host Nayib Bukele Following Deportati...  Date not found   \n",
       "\n",
       "                                                body  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  Money, for most people, is something they don‚Äô...  \n",
       "3  According to @rovercrc, former President Trump...  \n",
       "4  by Bloomberg, follows Bukele‚Äôs agreement to de...  \n",
       "5  El Salvador‚Äôs President Nayib Bukele made news...  \n",
       "6  The two pro-Bitcoin leaders have maintained co...  \n",
       "7  Panama is set to introduce a comprehensive dra...  \n",
       "8                                                     \n",
       "9  Trump plans to host El Salvador's President Bu...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def urls_to_table(urls):\n",
    "    \"\"\"Extracts text list of urls returns a table filled with urls, article titles, and body text.\"\"\"\n",
    "    \n",
    "    url_title_date_text = [extract_website_details(url) for url in urls]\n",
    "\n",
    "    cleaned_texts = [text[2] for text in url_title_date_text]\n",
    "    dates = [text[1] for text in url_title_date_text]\n",
    "    titles = [text[0] for text in url_title_date_text]\n",
    "\n",
    "    #[clean_extracted_text(text, section='header') for text in raw_url_text]\n",
    "    #cleaned_texts = [clean_extracted_text(text, section='body') for text in raw_url_text]\n",
    "\n",
    "    extracted_info = pd.DataFrame({\n",
    "        'url': urls,\n",
    "        'title': titles,\n",
    "        'date': dates,\n",
    "        'body': cleaned_texts\n",
    "    })\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "#demonstrates the function with the first file in the folder\n",
    "urls_1 = urls_in_file('eml_files/Google Alert - _Bitcoin_ _El Salvador__1.eml')\n",
    "mineria_1 = urls_to_table(urls_1)\n",
    "mineria_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9749a6c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m urls_to_table(urls)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#demonstrates the function with the full mineria google alert folder\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#takes 12-18 minutes to run it may be helpful to comment out\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m example_table = \u001b[43malert_folder_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meml_files/1. El Salvador Mineria 1-94 eml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m example_table[:\u001b[32m3\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36malert_folder_to_table\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Extracts text from all .eml files in a folder and returns a table filled with urls, article titles, and body text.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m urls = [url \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls_in_folder(file_path)]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murls_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43murls\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36murls_to_table\u001b[39m\u001b[34m(urls)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34murls_to_table\u001b[39m(urls):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extracts text list of urls returns a table filled with urls, article titles, and body text.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     url_title_date_text = \u001b[43m[\u001b[49m\u001b[43mextract_website_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43murls\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m     cleaned_texts = [text[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m url_title_date_text]\n\u001b[32m      7\u001b[39m     dates = [text[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m url_title_date_text]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34murls_to_table\u001b[39m(urls):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extracts text list of urls returns a table filled with urls, article titles, and body text.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     url_title_date_text = [\u001b[43mextract_website_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls]\n\u001b[32m      6\u001b[39m     cleaned_texts = [text[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m url_title_date_text]\n\u001b[32m      7\u001b[39m     dates = [text[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m url_title_date_text]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mextract_website_details\u001b[39m\u001b[34m(url, cutoff)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTakes in a url as a string and returns the text content of the webpage, removing paragraphs shorter than cutoff words.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mReturns a list containing the title, date, and body text of the webpage.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mIf the webpage cannot be accessed, returns a list of None values.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     response.raise_for_status()\n\u001b[32m     10\u001b[39m     soup = BeautifulSoup(response.content, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m#creates a BeautifulSoup object from the webpage content\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp_py3_11/lib/python3.11/ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def alert_folder_to_table(file_path):\n",
    "    \"\"\"Extracts text from all .eml files in a folder and returns a table filled with urls, article titles, and body text.\"\"\"\n",
    "    \n",
    "    urls = [url for url in urls_in_folder(file_path)]\n",
    "\n",
    "    return urls_to_table(urls)\n",
    "\n",
    "#demonstrates the function with the full mineria google alert folder\n",
    "\n",
    "#takes 12-18 minutes to run it may be helpful to comment out\n",
    "example_table = alert_folder_to_table('eml_files/1. El Salvador Mineria 1-94 eml')\n",
    "example_table[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e965c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports example_table to a csv file\n",
    "#example_table.to_csv('export_csv_files/example_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519f028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fxguys lets users trade without checking identity',\n",
       " 'national asset built trust among',\n",
       " '1 listing price ozak ai',\n",
       " 'president nayib bukele made news',\n",
       " 'framework helps investors earn regularly',\n",
       " 'yield real financial gains rather',\n",
       " 'high growth potential besides bitcoin',\n",
       " 'one main trait distinguishing fxguys',\n",
       " 'fxguys users get immediate benefits',\n",
       " 'investors seek options beyond long',\n",
       " 'profits plus broker trading revenue',\n",
       " 'lets users earn rewards',\n",
       " 'fxguys gives traders direct chances',\n",
       " 'top prop trading companies',\n",
       " 'fxg tokens gives users',\n",
       " 'fxguys platform gives access',\n",
       " 'investors direct financial rewards',\n",
       " 'price rises like bitcoin',\n",
       " 'prop trading funding program',\n",
       " 'offers clear financial growth',\n",
       " 'rewards users actively',\n",
       " 'every trade made',\n",
       " 'fxguys gives traders',\n",
       " 'several trade terminals',\n",
       " 'international monetary fund',\n",
       " 'fxg token costs',\n",
       " 'el salvador sticks',\n",
       " 'direct money gain',\n",
       " 'demand plus liquidity',\n",
       " 'trade2earn method gives',\n",
       " 'el salvador ‚Äô',\n",
       " 'el salvador ‚Äô',\n",
       " 'investors see fxguys',\n",
       " 'term price guessing',\n",
       " 'stage 3 presale',\n",
       " 'one major problem',\n",
       " 'kyc trade setup',\n",
       " 'exchange withdrawal fees',\n",
       " 'bukele buys bitcoin',\n",
       " 'bukele adds bitcoin',\n",
       " 'term price plan',\n",
       " 'new trading platform',\n",
       " 'standard crypto investments',\n",
       " 'reduce inflation risk',\n",
       " 'need trade choice',\n",
       " 'merely holding coins',\n",
       " 'staking trading prizes',\n",
       " 'follow bitcoin ‚Äô',\n",
       " 'presale success signals',\n",
       " 'makes fxguys one',\n",
       " 'increasing government control',\n",
       " 'bitcoin holders wait',\n",
       " '4 million raised',\n",
       " 'price growth',\n",
       " 'requires users',\n",
       " 'fxg tokens',\n",
       " 'bukele ‚Äô',\n",
       " 'financial plan',\n",
       " 'presale price',\n",
       " 'direct funds',\n",
       " 'financial system',\n",
       " 'fxguys offers',\n",
       " 'long term',\n",
       " 'staking gains',\n",
       " 'price rise',\n",
       " 'higher gains',\n",
       " 'fxguys shows',\n",
       " 'fxguys shows',\n",
       " 'fxguys removes',\n",
       " 'fxguys emerges',\n",
       " 'fxguys creates',\n",
       " 'fxguys changes',\n",
       " '4 million',\n",
       " 'trading funds',\n",
       " 'trading capital',\n",
       " 'trading bonuses',\n",
       " 'quiet investors',\n",
       " 'investors worry',\n",
       " 'investors look',\n",
       " 'investors look',\n",
       " 'fxguys stands',\n",
       " 'get returns',\n",
       " 'new opportunity',\n",
       " 'kyc rules',\n",
       " 'government limits',\n",
       " 'fxguys also',\n",
       " 'digital money',\n",
       " 'traders keep',\n",
       " 'network fees',\n",
       " 'charge fees',\n",
       " 'active traders',\n",
       " 'bitcoin keeps',\n",
       " 'transaction delays',\n",
       " 'stays unstable',\n",
       " 'services mentioned',\n",
       " 'roi promised',\n",
       " 'reward system',\n",
       " 'opinions expressed',\n",
       " 'one chance',\n",
       " 'necessarily represent',\n",
       " 'markets change',\n",
       " 'keep value',\n",
       " 'hold funds',\n",
       " 'friendly environments',\n",
       " 'first system',\n",
       " 'firm support',\n",
       " 'facing limits',\n",
       " 'educational purposes',\n",
       " 'dismissing warnings',\n",
       " 'digital reserve',\n",
       " 'currently priced',\n",
       " 'cryptocurrency investment',\n",
       " 'crypto reporter',\n",
       " 'crypto reporter',\n",
       " 'crypto reporter',\n",
       " 'completely decentralized',\n",
       " 'central platforms',\n",
       " 'central exchanges',\n",
       " 'better investment',\n",
       " 'adoption trends',\n",
       " 'stands apart',\n",
       " 'market cycles',\n",
       " 'defi market',\n",
       " 'loss caused',\n",
       " 'leading projects',\n",
       " 'content provider',\n",
       " 'trade',\n",
       " 'trade',\n",
       " 'plus',\n",
       " 'options',\n",
       " 'fxguys',\n",
       " 'trading',\n",
       " 'depend solely',\n",
       " 'investors',\n",
       " 'investors',\n",
       " 'platform',\n",
       " 'long',\n",
       " 'get',\n",
       " 'one',\n",
       " 'one',\n",
       " 'clear',\n",
       " 'traders',\n",
       " 'traders',\n",
       " 'profits',\n",
       " 'profits',\n",
       " 'offers',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'bitcoin',\n",
       " 'success',\n",
       " 'staking',\n",
       " 'risk',\n",
       " 'raised',\n",
       " 'program',\n",
       " 'program',\n",
       " 'method',\n",
       " 'makes',\n",
       " 'holding',\n",
       " 'holders',\n",
       " 'crypto',\n",
       " 'control',\n",
       " 'choice',\n",
       " 'stands',\n",
       " 'market',\n",
       " 'solely',\n",
       " 'projects',\n",
       " 'content',\n",
       " 'chance',\n",
       " 'caused',\n",
       " 'also',\n",
       " 'depend',\n",
       " 'depend',\n",
       " 'xrp',\n",
       " 'worries',\n",
       " 'wish',\n",
       " 'way',\n",
       " 'views',\n",
       " 'usually',\n",
       " 'use',\n",
       " 'use',\n",
       " 'trustworthiness',\n",
       " 'trader',\n",
       " 'trader',\n",
       " 'take',\n",
       " 'suits',\n",
       " 'statements',\n",
       " 'state',\n",
       " 'starts',\n",
       " 'short',\n",
       " 'share',\n",
       " 'selling',\n",
       " 'sell',\n",
       " 'role',\n",
       " 'responsible',\n",
       " 'responsible',\n",
       " 'reserves',\n",
       " 'research',\n",
       " 'reliance',\n",
       " 'ready',\n",
       " 'quality',\n",
       " 'provided',\n",
       " 'popular',\n",
       " 'offering',\n",
       " 'next',\n",
       " 'much',\n",
       " 'mt5',\n",
       " 'merging',\n",
       " 'matter',\n",
       " 'materials',\n",
       " 'match',\n",
       " 'less',\n",
       " 'involvement',\n",
       " 'invest',\n",
       " 'indirectly',\n",
       " 'idea',\n",
       " 'hopes',\n",
       " 'grows',\n",
       " 'goods',\n",
       " 'expand',\n",
       " 'eth',\n",
       " 'economy',\n",
       " 'dxtrade',\n",
       " 'directly',\n",
       " 'depends',\n",
       " 'damage',\n",
       " 'cycle',\n",
       " 'ctrader',\n",
       " 'contrast',\n",
       " 'connection',\n",
       " 'comparison',\n",
       " 'choose',\n",
       " 'buying',\n",
       " 'buying',\n",
       " 'buy',\n",
       " 'btc',\n",
       " 'brave',\n",
       " 'assets',\n",
       " 'article',\n",
       " 'article',\n",
       " 'article',\n",
       " 'article',\n",
       " 'approach',\n",
       " 'alleged',\n",
       " 'accuracy',\n",
       " '500',\n",
       " '2000',\n",
       " '20',\n",
       " '10',\n",
       " '05',\n",
       " '05',\n",
       " '000',\n",
       " '0',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_keywords_from_text_PREVIOUS(text, language='english', ignore=[]):\n",
    "    \"\"\" \n",
    "    Extracts keywords from a given text, ignoring specified keywords.\n",
    "    Not working yet... so far just kind of spews nonsense for some reason\n",
    "    \"\"\"\n",
    "    rake = Rake(language=language)\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    top_phrases = rake.get_ranked_phrases()\n",
    "    return top_phrases\n",
    "\n",
    "trial_url = urls_in_file('eml_files/Google Alert - _Bitcoin_ _El Salvador__1.eml')[4]\n",
    "trial_text = extract_website_details(trial_url)[2]\n",
    "trial_keywords = extract_keywords_from_text(trial_text)\n",
    "trial_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61043758",
   "metadata": {},
   "source": [
    "### Updated keyword extraction that uses a Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62ea29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "def extract_keywords_from_text(texts, top_k=50, ngram_range=(1,2), language=\"spanish\", min_df=2, max_df=0.9):\n",
    "    \"\"\"\n",
    "    Return top_k (term, count) from a corpus using CountVectorizer.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(stop_words=language, ngram_range=ngram_range, min_df=min_df, max_df=max_df)\n",
    "    X = vec.fit_transform(pd.Series(texts).fillna(\"\"))\n",
    "\n",
    "    counts = np.asarray(X.sum(axis=0)).ravel()\n",
    "    terms  = np.array(vec.get_feature_names_out())\n",
    "    order  = counts.argsort()[::-1][:top_k]\n",
    "    out = pd.DataFrame({\"term\": terms[order], \"count\": counts[order]}).reset_index(drop=True)\n",
    "    print(f\"[INFO] Extracted top {len(out)} terms.\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db90a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_py3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
